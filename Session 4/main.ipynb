{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "517d6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cc363",
   "metadata": {},
   "source": [
    "preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "861f691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # strip buat ngapus space di blkg\n",
    "    sentence = re.sub(r\"[(?.,!)]\", \" \\1 \", sentence) #nambah spasi di depan d blkg simbol tadi\n",
    "    sentence = re.sub(r'[\"]+', \" \", sentence) # ganti petik dua jadi spasi\n",
    "    sentence = re.sub(r\"[^a-z?,.!]+\", \" \", sentence) #menghapus simbol\" lain\n",
    "    sentence = sentence.strip() #hapus spasi berlebihan\n",
    "    sentence = '<start> ' + sentence + ' <end>' #kasih tau kapan sentence mulai dan akhir\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9165bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns: ['English', 'French']\n",
      "Input Data: ['<start> go <end>', '<start> go <end>', '<start> go <end>', '<start> go <end>', '<start> hi <end>']\n",
      "Target Data: ['<start> va <end>', '<start> marche <end>', '<start> en route <end>', '<start> bouge <end>', '<start> salut <end>']\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path, num_examples = None):\n",
    "    dataset = pd.read_csv(path) #baca \n",
    "    if num_examples: #ngecek kalo ada dikasi num examples\n",
    "        dataset = dataset.head(num_examples) #slicing\n",
    "\n",
    "    print(f\"Dataset Columns: {dataset.columns.tolist()}\")\n",
    "\n",
    "    input_data = [preprocessing_sentence(sentence) for sentence in dataset [\"English\"]]\n",
    "    target_data = [preprocessing_sentence(sentence) for sentence in dataset [\"French\"]]\n",
    "    return input_data, target_data\n",
    "\n",
    "path = \"./english_french.csv\"\n",
    "num_examples = 100\n",
    "\n",
    "input_data, target_data = load_dataset(path, num_examples)\n",
    "print(f\"Input Data: {input_data[:5]}\")\n",
    "print(f\"Target Data: {target_data[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf402027",
   "metadata": {},
   "source": [
    "word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa9c4e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Words: {'<start>': 1, '<end>': 2, 'run': 3, 'relax': 4, 'go': 5, 'toi': 6, 'wait': 7, 'hello': 8, 'i': 9, 'it': 10, 'l': 11, 'salut': 12, 'stop': 13, 'attack': 14, 'buy': 15, 'cheers': 16, 'cours': 17, 'vos': 18, 'vous': 19, 'attendez': 20, 'd': 21, 'le': 22, 'wow': 23, 'duck': 24, 'on': 25, 'won': 26, 'smile': 27, 'get': 28, 'up': 29, 'now': 30, 'te': 31, 'attends': 32, 'bonjour': 33, 'j': 34, 'ai': 35, 'calme': 36, 'tends': 37, 'la': 38, 'attaque': 39, 'maintenant': 40, 'hi': 41, 'hide': 42, 'jump': 43, 'begin': 44, 'see': 45, 'oh': 46, 'eat': 47, 'va': 48, 'courez': 49, 'prenez': 50, 'jambes': 51, 'cous': 52, 'file': 53, 'filez': 54, 'fuyez': 55, 'fuyons': 56, 'a': 57, 'saute': 58, 'je': 59, 'gagn': 60, 'relaxe': 61, 'du': 62, 'souriez': 63, 'achetez': 64, 'ach': 65, 'sant': 66, 'tchin': 67, 've': 68, 'y': 69, 'who': 70, 'fire': 71, 'help': 72, 'try': 73, 'no': 74, 'sorry': 75, 'exhale': 76, 'marche': 77, 'en': 78, 'route': 79, 'bouge': 80, 'qui': 81, 'alors': 82, 'waouh': 83, 'wah': 84, 'terre': 85, 'baisse': 86, 'baissez': 87, 'au': 88, 'feu': 89, 'aide': 90, 'cache': 91, 'cachez': 92, 'suffit': 93, 'arr': 94, 'commencez': 95, 'commence': 96, 'poursuis': 97, 'continuez': 98, 'poursuivez': 99, 'comprends': 100, 'aha': 101, 'essaye': 102, 'emport': 103, 'non': 104, 'tendez': 105, 'max': 106, 'cool': 107, 'raoul': 108, 'calmez': 109, 'tranquille': 110, 'souris': 111, 'pour': 112, 'cam': 113, 'ra': 114, 'pardon': 115, 'attaquez': 116, 'votre': 117, 'merci': 118, 'mangez': 119, 'mange': 120, 'expire': 121, 'debout': 122, 'allez': 123, 'vas': 124}\n",
      "Input Tensor: [[ 1  5  2  0]\n",
      " [ 1  5  2  0]\n",
      " [ 1  5  2  0]\n",
      " [ 1  5  2  0]\n",
      " [ 1 41  2  0]\n",
      " [ 1 41  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1 70  2  0]\n",
      " [ 1 23  2  0]\n",
      " [ 1 23  2  0]\n",
      " [ 1 23  2  0]\n",
      " [ 1 24  2  0]\n",
      " [ 1 24  2  0]\n",
      " [ 1 24  2  0]\n",
      " [ 1 71  2  0]\n",
      " [ 1 72  2  0]\n",
      " [ 1 42  2  0]\n",
      " [ 1 42  2  0]\n",
      " [ 1 43  2  0]\n",
      " [ 1 43  2  0]\n",
      " [ 1 13  2  0]\n",
      " [ 1 13  2  0]\n",
      " [ 1 13  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1 44  2  0]\n",
      " [ 1 44  2  0]\n",
      " [ 1  5 25  2]\n",
      " [ 1  5 25  2]\n",
      " [ 1  5 25  2]\n",
      " [ 1  8  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1  9 45  2]\n",
      " [ 1  9 45  2]\n",
      " [ 1  9 73  2]\n",
      " [ 1  9 26  2]\n",
      " [ 1  9 26  2]\n",
      " [ 1  9 26  2]\n",
      " [ 1 46 74  2]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1 27  2  0]\n",
      " [ 1 27  2  0]\n",
      " [ 1 27  2  0]\n",
      " [ 1 75  2  0]\n",
      " [ 1 14  2  0]\n",
      " [ 1 14  2  0]\n",
      " [ 1 14  2  0]\n",
      " [ 1 14  2  0]\n",
      " [ 1 15 10  2]\n",
      " [ 1 15 10  2]\n",
      " [ 1 15 10  2]\n",
      " [ 1 15 10  2]\n",
      " [ 1 16  2  0]\n",
      " [ 1 16  2  0]\n",
      " [ 1 16  2  0]\n",
      " [ 1 16  2  0]\n",
      " [ 1 47 10  2]\n",
      " [ 1 47 10  2]\n",
      " [ 1 76  2  0]\n",
      " [ 1 28 29  2]\n",
      " [ 1 28 29  2]\n",
      " [ 1 28 29  2]\n",
      " [ 1  5 30  2]\n",
      " [ 1  5 30  2]\n",
      " [ 1  5 30  2]]\n",
      "Target Tensor: [[  1  48   2   0   0   0   0]\n",
      " [  1  77   2   0   0   0   0]\n",
      " [  1  78  79   2   0   0   0]\n",
      " [  1  80   2   0   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1  49   2   0   0   0   0]\n",
      " [  1  50  18  51  18  52   2]\n",
      " [  1  53   2   0   0   0   0]\n",
      " [  1  54   2   0   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1  55   2   0   0   0   0]\n",
      " [  1  56   2   0   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1  49   2   0   0   0   0]\n",
      " [  1  50  18  51  18  52   2]\n",
      " [  1  53   2   0   0   0   0]\n",
      " [  1  54   2   0   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1  55   2   0   0   0   0]\n",
      " [  1  56   2   0   0   0   0]\n",
      " [  1  81   2   0   0   0   0]\n",
      " [  1  57  82   2   0   0   0]\n",
      " [  1  83   2   0   0   0   0]\n",
      " [  1  84   2   0   0   0   0]\n",
      " [  1  85   2   0   0   0   0]\n",
      " [  1  86   6   2   0   0   0]\n",
      " [  1  87  19   2   0   0   0]\n",
      " [  1  88  89   2   0   0   0]\n",
      " [  1  11  90   2   0   0   0]\n",
      " [  1  91   6   2   0   0   0]\n",
      " [  1  92  19   2   0   0   0]\n",
      " [  1  58   2   0   0   0   0]\n",
      " [  1  58   2   0   0   0   0]\n",
      " [  1  57  93   2   0   0   0]\n",
      " [  1  13   2   0   0   0   0]\n",
      " [  1  94  31   6   2   0   0]\n",
      " [  1  32   2   0   0   0   0]\n",
      " [  1  20   2   0   0   0   0]\n",
      " [  1  20   2   0   0   0   0]\n",
      " [  1  32   2   0   0   0   0]\n",
      " [  1  20   2   0   0   0   0]\n",
      " [  1  32   2   0   0   0   0]\n",
      " [  1  20   2   0   0   0   0]\n",
      " [  1  95   2   0   0   0   0]\n",
      " [  1  96   2   0   0   0   0]\n",
      " [  1  97   2   0   0   0   0]\n",
      " [  1  98   2   0   0   0   0]\n",
      " [  1  99   2   0   0   0   0]\n",
      " [  1  33   2   0   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  33   2   0   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  33   2   0   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  59 100   2   0   0   0]\n",
      " [  1 101   2   0   0   0   0]\n",
      " [  1  34 102   2   0   0   0]\n",
      " [  1  34  35  60   2   0   0]\n",
      " [  1  59  11  35 103   2   0]\n",
      " [  1  34  35  60   2   0   0]\n",
      " [  1  46 104   2   0   0   0]\n",
      " [  1  36   6   2   0   0   0]\n",
      " [  1  21  37   6   2   0   0]\n",
      " [  1  21 105  19   2   0   0]\n",
      " [  1  61 106   2   0   0   0]\n",
      " [  1 107 108   2   0   0   0]\n",
      " [  1  62  36   2   0   0   0]\n",
      " [  1  61   2   0   0   0   0]\n",
      " [  1 109  19   2   0   0   0]\n",
      " [  1  21  37   6   2   0   0]\n",
      " [  1  21  37   6   2   0   0]\n",
      " [  1  62  36   2   0   0   0]\n",
      " [  1 110   2   0   0   0   0]\n",
      " [  1  63   2   0   0   0   0]\n",
      " [  1 111 112  38 113 114   2]\n",
      " [  1  63   2   0   0   0   0]\n",
      " [  1 115   2   0   0   0   0]\n",
      " [  1  39   2   0   0   0   0]\n",
      " [  1 116   2   0   0   0   0]\n",
      " [  1  11  39   2   0   0   0]\n",
      " [  1  11  39   2   0   0   0]\n",
      " [  1  64  38   2   0   0   0]\n",
      " [  1  65  31  22   2   0   0]\n",
      " [  1  64  22   2   0   0   0]\n",
      " [  1  65  31  38   2   0   0]\n",
      " [  1  66   2   0   0   0   0]\n",
      " [  1 117  66   2   0   0   0]\n",
      " [  1 118   2   0   0   0   0]\n",
      " [  1  67  67   2   0   0   0]\n",
      " [  1 119  22   2   0   0   0]\n",
      " [  1 120  22   2   0   0   0]\n",
      " [  1 121   2   0   0   0   0]\n",
      " [  1  11  68   6   2   0   0]\n",
      " [  1  11  68   6   2   0   0]\n",
      " [  1 122   2   0   0   0   0]\n",
      " [  1  48  40   2   0   0   0]\n",
      " [  1 123  69  40   2   0   0]\n",
      " [  1 124  69  40   2   0   0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(input_data + target_data)\n",
    "\n",
    "#ngubah kata jadi vektor\n",
    "input_tensor = tokenizer.texts_to_sequences(input_data)\n",
    "target_tensor = tokenizer.texts_to_sequences(target_data)\n",
    "\n",
    "#cari vektor dengan length terpanjang\n",
    "max_input_length = max(len(seq) for seq in input_tensor)\n",
    "max_target_length = max(len(seq) for seq in target_tensor)\n",
    "\n",
    "#padding vector biar ukurannya sama besar\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen = max_input_length, padding= 'post')\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, max_target_length, padding='post')\n",
    "\n",
    "print(f\"Tokenizer Words: {tokenizer.word_index}\")\n",
    "print(f\"Input Tensor: {input_tensor}\")\n",
    "print(f\"Target Tensor: {target_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db97ea",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a89f904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(64, 4), dtype=tf.int32, name=None), TensorSpec(shape=(64, 7), dtype=tf.int32, name=None))>\n",
      "Input Sample: [[ 1 14  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  5  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  9 26  2]\n",
      " [ 1 16  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1 15 10  2]\n",
      " [ 1 75  2  0]\n",
      " [ 1  5 25  2]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1 76  2  0]\n",
      " [ 1 13  2  0]\n",
      " [ 1  9 26  2]\n",
      " [ 1  7  2  0]\n",
      " [ 1  9 45  2]\n",
      " [ 1  8  2  0]\n",
      " [ 1 44  2  0]\n",
      " [ 1  7  2  0]\n",
      " [ 1 47 10  2]\n",
      " [ 1  8  2  0]\n",
      " [ 1  5  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1 28 29  2]\n",
      " [ 1 43  2  0]\n",
      " [ 1 46 74  2]\n",
      " [ 1  3  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1 23  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1 14  2  0]\n",
      " [ 1 14  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1 24  2  0]\n",
      " [ 1  8  2  0]\n",
      " [ 1 23  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1 42  2  0]\n",
      " [ 1 28 29  2]\n",
      " [ 1  7  2  0]\n",
      " [ 1 15 10  2]\n",
      " [ 1  4  2  0]\n",
      " [ 1 28 29  2]\n",
      " [ 1  3  2  0]\n",
      " [ 1 71  2  0]\n",
      " [ 1  5 25  2]\n",
      " [ 1  5  2  0]\n",
      " [ 1  3  2  0]\n",
      " [ 1  5 30  2]\n",
      " [ 1 16  2  0]\n",
      " [ 1  5 30  2]\n",
      " [ 1 24  2  0]\n",
      " [ 1  9 73  2]\n",
      " [ 1 15 10  2]\n",
      " [ 1 13  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  4  2  0]\n",
      " [ 1  5 25  2]\n",
      " [ 1  7  2  0]]\n",
      "Target Sample: [[  1  11  39   2   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  55   2   0   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1  80   2   0   0   0   0]\n",
      " [  1  53   2   0   0   0   0]\n",
      " [  1  59  11  35 103   2   0]\n",
      " [  1 117  66   2   0   0   0]\n",
      " [  1  53   2   0   0   0   0]\n",
      " [  1  65  31  22   2   0   0]\n",
      " [  1 115   2   0   0   0   0]\n",
      " [  1  97   2   0   0   0   0]\n",
      " [  1  21  37   6   2   0   0]\n",
      " [  1  21  37   6   2   0   0]\n",
      " [  1 121   2   0   0   0   0]\n",
      " [  1  94  31   6   2   0   0]\n",
      " [  1  34  35  60   2   0   0]\n",
      " [  1  32   2   0   0   0   0]\n",
      " [  1 101   2   0   0   0   0]\n",
      " [  1  33   2   0   0   0   0]\n",
      " [  1  96   2   0   0   0   0]\n",
      " [  1  32   2   0   0   0   0]\n",
      " [  1 120  22   2   0   0   0]\n",
      " [  1  12   2   0   0   0   0]\n",
      " [  1  78  79   2   0   0   0]\n",
      " [  1  33   2   0   0   0   0]\n",
      " [  1 122   2   0   0   0   0]\n",
      " [  1  58   2   0   0   0   0]\n",
      " [  1  46 104   2   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1 110   2   0   0   0   0]\n",
      " [  1  56   2   0   0   0   0]\n",
      " [  1  21 105  19   2   0   0]\n",
      " [  1  83   2   0   0   0   0]\n",
      " [  1  49   2   0   0   0   0]\n",
      " [  1  11  39   2   0   0   0]\n",
      " [  1  39   2   0   0   0   0]\n",
      " [  1  49   2   0   0   0   0]\n",
      " [  1  87  19   2   0   0   0]\n",
      " [  1  33   2   0   0   0   0]\n",
      " [  1  57  82   2   0   0   0]\n",
      " [  1  17   2   0   0   0   0]\n",
      " [  1  92  19   2   0   0   0]\n",
      " [  1  11  68   6   2   0   0]\n",
      " [  1  20   2   0   0   0   0]\n",
      " [  1  64  22   2   0   0   0]\n",
      " [  1  36   6   2   0   0   0]\n",
      " [  1  11  68   6   2   0   0]\n",
      " [  1  50  18  51  18  52   2]\n",
      " [  1  88  89   2   0   0   0]\n",
      " [  1  99   2   0   0   0   0]\n",
      " [  1  77   2   0   0   0   0]\n",
      " [  1  50  18  51  18  52   2]\n",
      " [  1  48  40   2   0   0   0]\n",
      " [  1  66   2   0   0   0   0]\n",
      " [  1 123  69  40   2   0   0]\n",
      " [  1  85   2   0   0   0   0]\n",
      " [  1  34 102   2   0   0   0]\n",
      " [  1  65  31  38   2   0   0]\n",
      " [  1  13   2   0   0   0   0]\n",
      " [  1  61   2   0   0   0   0]\n",
      " [  1  61 106   2   0   0   0]\n",
      " [  1  98   2   0   0   0   0]\n",
      " [  1  20   2   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#train:test -> 80 : 20\n",
    "input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(\n",
    "    input_tensor, \n",
    "    target_tensor, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "batch_size = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)) #gabungin data train input dan target\n",
    "dataset = dataset.shuffle(BUFFER_SIZE) #shuffle data\n",
    "dataset = dataset.batch(batch_size, drop_remainder = True) #mcah jadi batches\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "for sample in dataset.take(1):\n",
    "    input_sample, target_sample = sample\n",
    "    print(f\"Input Sample: {input_sample}\")\n",
    "    print(f\"Target Sample: {target_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad157363",
   "metadata": {},
   "source": [
    "create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1a09c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model): #bikin class turunan tf.keras.Model\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size): #constructor\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #ngubah vector sebelumnya jadi lebih 'bermakna'\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.enc_units, \n",
    "            return_sequences=True, #ngembaliin seluruh kalimat, bkn kalimat akhir doang\n",
    "            return_state=True,  #hidden state model\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x) #ngubah jadi vector yang ada semanticsnya\n",
    "        output, hidden_state = self.gru(x, initial_state = hidden)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186cec5",
   "metadata": {},
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2734b3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output Shape (batch_sze, seq length, units): (64, 4, 1024)\n",
      "Encoder Hidden State Shape: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "enc_units = 1024\n",
    "batch_size = 64\n",
    "\n",
    "encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "input_sample = input_tensor_train[:batch_size]\n",
    "\n",
    "sample_output, sample_hidden = encoder(input_sample, sample_hidden)\n",
    "\n",
    "print(f\"Encoder Output Shape (batch_sze, seq length, units): {sample_output.shape}\")\n",
    "print(f\"Encoder Hidden State Shape: {sample_hidden.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
