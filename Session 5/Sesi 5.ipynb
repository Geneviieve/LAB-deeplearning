{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = tf.data.TextLineDataset(\"ind.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = 1000\n",
    "ENGLISH_SEQUENCE_LENGTH = 32\n",
    "FRENCH_SEQUENCE_LENGTH = 32\n",
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization Layer\n",
    "english_vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = ENGLISH_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "french_vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = FRENCH_SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Text\n",
    "def split_text(text):\n",
    "    text = tf.strings.split(text, '\\t')\n",
    "    input_1 = text[:1]\n",
    "    input_2 = 'starttoken ' + text[1:2] + ' endtoken'\n",
    "    return input_1, input_2\n",
    "\n",
    "def vectorize(text):\n",
    "    text = tf.strings.split(text, '\\t')\n",
    "    input_1 = text[:1]\n",
    "    start_input = 'starttoken ' + text[1:2]\n",
    "    end_input = text[1:2] + ' endtoken'\n",
    "    print(f\"Vectorization -- Start Input: {start_input} End Input: {end_input}\")\n",
    "    return {\n",
    "        'input_1': english_vectorization_layer(input_1),\n",
    "        'input_2': french_vectorization_layer(start_input)\n",
    "    }, french_vectorization_layer(end_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating english training data and vectorization layer...\n",
      "Creating french training data and vectorization layer...\n",
      "Vectorization -- Start Input: Tensor(\"add:0\", shape=(None,), dtype=string) End Input: Tensor(\"add_1:0\", shape=(None,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "splitted_dataset = text_dataset.map(split_text)\n",
    "\n",
    "# Create training data\n",
    "print(\"Creating english training data and vectorization layer...\")\n",
    "english_training_data = splitted_dataset.map(lambda x, y: x)\n",
    "english_vectorization_layer.adapt(english_training_data)\n",
    "\n",
    "print(\"Creating french training data and vectorization layer...\")\n",
    "french_training_data = splitted_dataset.map(lambda x, y: y)\n",
    "french_vectorization_layer.adapt(french_training_data)\n",
    "\n",
    "# Map Shuffle\n",
    "dataset = text_dataset.map(vectorize)\n",
    "\n",
    "# Shuffling dataset and Batching dataset\n",
    "dataset = dataset.shuffle(200).unbatch().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: '[UNK]', 2: 'starttoken', 3: 'endtoken', 4: 'tom', 5: 'aku', 6: 'tidak', 7: 'yang', 8: 'di', 9: 'saya', 10: 'kamu', 11: 'itu', 12: 'ini', 13: 'dia', 14: 'apa', 15: 'akan', 16: 'ada', 17: 'bisa', 18: 'dengan', 19: 'ke', 20: 'kita', 21: 'tahu', 22: 'kami', 23: 'mary', 24: 'apakah', 25: 'untuk', 26: 'ingin', 27: 'pergi', 28: 'kau', 29: 'adalah', 30: 'sudah', 31: 'anda', 32: 'suka', 33: 'harus', 34: 'dan', 35: 'lebih', 36: 'dari', 37: 'sedang', 38: 'punya', 39: 'orang', 40: 'mereka', 41: 'sangat', 42: 'makan', 43: 'mana', 44: 'sini', 45: 'dalam', 46: 'banyak', 47: 'hari', 48: 'pada', 49: 'lagi', 50: 'jangan', 51: 'pernah', 52: 'semua', 53: 'melakukan', 54: 'rumah', 55: 'seorang', 56: 'siapa', 57: 'bahasa', 58: 'bukan', 59: 'mau', 60: 'melihat', 61: 'masih', 62: 'telah', 63: 'tahun', 64: 'berapa', 65: 'hal', 66: 'saja', 67: 'belum', 68: 'tolong', 69: 'buku', 70: 'tiga', 71: 'tinggal', 72: 'kalau', 73: 'datang', 74: 'sebuah', 75: 'baik', 76: 'waktu', 77: 'mobil', 78: 'mungkin', 79: 'sekarang', 80: 'bahwa', 81: 'sesuatu', 82: 'seperti', 83: 'memiliki', 84: 'terlalu', 85: 'perlu', 86: 'menjadi', 87: 'membeli', 88: 'baru', 89: 'kalian', 90: 'bagaimana', 91: 'tidur', 92: 'satu', 93: 'bermain', 94: 'berbicara', 95: 'lakukan', 96: 'anak', 97: 'sekali', 98: 'terjadi', 99: 'salah', 100: 'kenapa', 101: 'jam', 102: 'hanya', 103: 'belajar', 104: 'selalu', 105: 'malam', 106: 'tentang', 107: 'terlihat', 108: 'ia', 109: 'mulai', 110: 'boston', 111: 'uang', 112: 'melakukannya', 113: 'membaca', 114: 'juga', 115: 'bekerja', 116: 'mari', 117: 'kemarin', 118: 'sendiri', 119: 'sana', 120: 'saat', 121: 'prancis', 122: 'kembali', 123: 'kapan', 124: 'benar', 125: 'atas', 126: 'minum', 127: 'bertemu', 128: 'sakit', 129: 'sama', 130: 'merasa', 131: 'lama', 132: 'dapat', 133: 'karena', 134: 'pikir', 135: 'menunggu', 136: 'daripada', 137: 'apaapa', 138: 'besok', 139: 'lalu', 140: 'setiap', 141: 'sekolah', 142: 'sampai', 143: 'bilang', 144: 'beberapa', 145: 'pulang', 146: 'kucing', 147: 'boleh', 148: 'mengapa', 149: 'kasih', 150: 'besar', 151: 'jika', 152: 'hidup', 153: 'masalah', 154: 'pun', 155: 'dua', 156: 'air', 157: 'selama', 158: 'membantu', 159: 'senang', 160: 'pagi', 161: 'duduk', 162: 'depan', 163: 'berhenti', 164: 'yakin', 165: 'membuat', 166: 'kali', 167: 'benarbenar', 168: 'mati', 169: 'bisakah', 170: 'kota', 171: 'katakan', 172: 'jepang', 173: 'dunia', 174: 'cukup', 175: 'terima', 176: 'berkata', 177: 'kepada', 178: 'jalan', 179: 'menemukan', 180: 'mencoba', 181: 'lain', 182: 'tempat', 183: 'ketika', 184: 'inggris', 185: 'panas', 186: 'luar', 187: 'hujan', 188: 'berada', 189: 'pukul', 190: 'padaku', 191: 'semuanya', 192: 'makanan', 193: 'berenang', 194: 'australia', 195: 'seharusnya', 196: 'meja', 197: 'lupa', 198: 'diri', 199: 'bagus', 200: 'tua', 201: 'tangan', 202: 'takut', 203: 'sering', 204: 'ruangan', 205: 'percaya', 206: 'kecil', 207: 'bulan', 208: 'anjing', 209: 'pekerjaan', 210: 'merokok', 211: 'hampir', 212: 'harap', 213: 'cepat', 214: 'atau', 215: 'tetapi', 216: 'teman', 217: 'pintu', 218: 'minggu', 219: 'memberi', 220: 'keluar', 221: 'tak', 222: 'sedikit', 223: 'masuk', 224: 'maaf', 225: 'tadi', 226: 'sibuk', 227: 'padamu', 228: 'meninggal', 229: 'mengatakan', 230: 'menangis', 231: 'menulis', 232: 'mendengar', 233: 'kereta', 234: 'guru', 235: 'dokter', 236: 'denganmu', 237: 'bersama', 238: 'sepertinya', 239: 'selamat', 240: 'sebelum', 241: 'merah', 242: 'kan', 243: 'jauh', 244: 'cara', 245: 'siang', 246: 'seekor', 247: 'rasa', 248: 'lihat', 249: 'ya', 250: 'tinggi', 251: 'tepat', 252: 'sepeda', 253: 'paling', 254: 'nama', 255: 'marah', 256: 'kamar', 257: 'dulu', 258: 'ayah', 259: 'turun', 260: 'terlambat', 261: 'seseorang', 262: 'musim', 263: 'butuh', 264: 'begitu', 265: 'bawah', 266: 'apel', 267: 'pasti', 268: 'oleh', 269: 'musik', 270: 'muda', 271: 'mencari', 272: 'lapar', 273: 'gunung', 274: 'dingin', 275: 'tersebut', 276: 'stasiun', 277: 'menyukai', 278: 'mengambil', 279: 'berikan', 280: 'tiba', 281: 'setelah', 282: 'kopi', 283: 'berbohong', 284: 'surat', 285: 'setuju', 286: 'penting', 287: 'menonton', 288: 'lakilaki', 289: 'bersalah', 290: 'apapun', 291: 'sulit', 292: 'perempuan', 293: 'memakai', 294: 'ulang', 295: 'tanpa', 296: 'siswa', 297: 'segera', 298: 'sebentar', 299: 'menggunakan', 300: 'mengerti', 301: 'bolehkah', 302: 'anakanak', 303: 'membutuhkan', 304: 'melihatnya', 305: 'keras', 306: 'biasanya', 307: 'berpikir', 308: 'ayahku', 309: 'anggur', 310: 'terakhir', 311: 'tapi', 312: 'selesai', 313: 'meminta', 314: 'kehilangan', 315: 'cinta', 316: 'cantik', 317: 'berangkat', 318: 'silakan', 319: 'rumahku', 320: 'padanya', 321: 'menit', 322: 'memakan', 323: 'jadi', 324: 'ikan', 325: 'ibu', 326: 'bangun', 327: 'sungai', 328: 'sebenarnya', 329: 'sebelumnya', 330: 'satusatunya', 331: 'puluh', 332: 'milik', 333: 'menyelesaikan', 334: 'menarik', 335: 'ingat', 336: 'film', 337: 'enam', 338: 'dekat', 339: 'berjalan', 340: 'bahagia', 341: 'toko', 342: 'putih', 343: 'pria', 344: 'mendengarkan', 345: 'lantai', 346: 'kesalahan', 347: 'berubah', 348: 'bercanda', 349: 'berat', 350: 'amerika', 351: 'tetap', 352: 'sepuluh', 353: 'pintunya', 354: 'pertama', 355: 'nanti', 356: 'lahir', 357: 'dengannya', 358: 'biarkan', 359: 'baikbaik', 360: 'awal', 361: 'terus', 362: 'tertarik', 363: 'sebaiknya', 364: 'pertanyaan', 365: 'orangorang', 366: 'negeri', 367: 'mudah', 368: 'mengetahui', 369: 'membuatku', 370: 'lelah', 371: 'buruk', 372: 'bertanya', 373: 'aneh', 374: 'tenis', 375: 'siap', 376: 'sendirian', 377: 'namanya', 378: 'menerima', 379: 'mahal', 380: 'kata', 381: 'jatuh', 382: 'ibuku', 383: 'tutup', 384: 'polisi', 385: 'pohon', 386: 'para', 387: 'menurutmu', 388: 'menikah', 389: 'indah', 390: 'gila', 391: 'berwarna', 392: 'berusaha', 393: 'berlari', 394: 'berhasil', 395: 'terkenal', 396: 'susu', 397: 'rapat', 398: 'oktober', 399: 'nasi', 400: 'meninggalkan', 401: 'menghabiskan', 402: 'kosong', 403: 'kanada', 404: 'hingga', 405: 'enak', 406: 'bunga', 407: 'bodoh', 408: 'bicara', 409: 'berbahasa', 410: 'benci', 411: 'ayo', 412: 'tertawa', 413: 'terbaik', 414: 'senin', 415: 'sekitar', 416: 'secara', 417: 'salju', 418: 'mobilnya', 419: 'kotak', 420: 'kelas', 421: 'kecelakaan', 422: 'buka', 423: 'berteriak', 424: 'tunggu', 425: 'sepanjang', 426: 'pesta', 427: 'peduli', 428: 'minta', 429: 'mengenakan', 430: 'mendapatkan', 431: 'menang', 432: 'membunuh', 433: 'memberitahu', 434: 'mata', 435: 'mabuk', 436: 'kira', 437: 'buah', 438: 'bola', 439: 'berdua', 440: 'bagi', 441: 'baca', 442: 'serius', 443: 'semalam', 444: 'pesawat', 445: 'negara', 446: 'memberitahuku', 447: 'lima', 448: 'kertas', 449: 'kerja', 450: 'denganku', 451: 'beli', 452: 'warna', 453: 'terbiasa', 454: 'televisi', 455: 'telepon', 456: 'taman', 457: 'tahan', 458: 'saling', 459: 'naik', 460: 'murah', 461: 'menjual', 462: 'mengajar', 463: 'membuka', 464: 'manusia', 465: 'kursi', 466: 'kedua', 467: 'biru', 468: 'beri', 469: 'berdiri', 470: 'agak', 471: 'terbuat', 472: 'sepatu', 473: 'sejak', 474: 'mengunjungi', 475: 'menelepon', 476: 'memasak', 477: 'mampu', 478: 'lagu', 479: 'kantor', 480: 'itulah', 481: 'ide', 482: 'hotel', 483: 'hitam', 484: 'es', 485: 'dirinya', 486: 'delapan', 487: 'bus', 488: 'adik', 489: 'tv', 490: 'terkejut', 491: 'sarapan', 492: 'penasaran', 493: 'pacar', 494: 'novel', 495: 'menuju', 496: 'memutuskan', 497: 'meminjam', 498: 'membayar', 499: 'ketimbang', 500: 'john', 501: 'inginkan', 502: 'harganya', 503: 'harga', 504: 'gadis', 505: 'dimulai', 506: 'daging', 507: 'coba', 508: 'bantuan', 509: 'bahkan', 510: 'ayahnya', 511: 'akhirnya', 512: 'terluka', 513: 'temanku', 514: 'sempurna', 515: 'sebagai', 516: 'saudara', 517: 'roti', 518: 'rahasia', 519: 'menyerah', 520: 'menyadari', 521: 'menjawab', 522: 'mendapat', 523: 'matematika', 524: 'mandi', 525: 'ketahui', 526: 'kemacetan', 527: 'istri', 528: 'ibunya', 529: 'dolar', 530: 'darah', 531: 'biasa', 532: 'berencana', 533: 'belas', 534: 'adikku', 535: 'tersenyum', 536: 'terhadap', 537: 'terbuka', 538: 'tanggal', 539: 'semakin', 540: 'sehat', 541: 'sebelah', 542: 'pisau', 543: 'mobilku', 544: 'menyukainya', 545: 'menutup', 546: 'menolak', 547: 'mengenal', 548: 'mempunyai', 549: 'membuatmu', 550: 'membawa', 551: 'melihatmu', 552: 'masa', 553: 'khawatir', 554: 'keretanya', 555: 'kepadamu', 556: 'kehabisan', 557: 'kecewa', 558: 'kaki', 559: 'jawaban', 560: 'gelap', 561: 'dilakukan', 562: 'danau', 563: 'cuma', 564: 'buta', 565: 'burung', 566: 'bukankah', 567: 'bintang', 568: 'tujuh', 569: 'tertidur', 570: 'telur', 571: 'teh', 572: 'tanganmu', 573: 'rumahnya', 574: 'rencana', 575: 'pikirkan', 576: 'piano', 577: 'panjang', 578: 'menyenangkan', 579: 'menyeberang', 580: 'mencuci', 581: 'memang', 582: 'malu', 583: 'langit', 584: 'kurang', 585: 'kuat', 586: 'kepala', 587: 'kepadaku', 588: 'kebingungan', 589: 'kabar', 590: 'hangat', 591: 'gigi', 592: 'eropa', 593: 'ekor', 594: 'bir', 595: 'berbeda', 596: 'bank', 597: 'api', 598: 'wanita', 599: 'tuaku', 600: 'tau', 601: 'taksi', 602: 'sepak', 603: 'sehari', 604: 'sedih', 605: 'rambut', 606: 'pintar', 607: 'pandai', 608: 'pakaian', 609: 'paham', 610: 'menunjukkan', 611: 'mengerjakan', 612: 'menari', 613: 'memilih', 614: 'membantumu', 615: 'memancing', 616: 'maukah', 617: 'lintas', 618: 'kulakukan', 619: 'keputusan', 620: 'keadaan', 621: 'kamera', 622: 'kakakku', 623: 'ikut', 624: 'hati', 625: 'emas', 626: 'dimakan', 627: 'bunuh', 628: 'bukanlah', 629: 'berbahaya', 630: 'bangunan', 631: 'badan', 632: 'arah', 633: 'ambil', 634: 'airnya', 635: 'agar', 636: '20', 637: 'waktunya', 638: 'tim', 639: 'soal', 640: 'seberapa', 641: 'pulau', 642: 'pernahkah', 643: 'permisi', 644: 'pemain', 645: 'pantai', 646: 'pakai', 647: 'pacarku', 648: 'nyalakan', 649: 'minuman', 650: 'mengingat', 651: 'menginap', 652: 'mengikuti', 653: 'mempercayai', 654: 'mawar', 655: 'lupakan', 656: 'kemungkinan', 657: 'keju', 658: 'kaya', 659: 'ibumu', 660: 'hujannya', 661: 'gitar', 662: 'gambar', 663: 'dirimu', 664: 'dimana', 665: 'dapur', 666: 'cuaca', 667: 'cerita', 668: 'caranya', 669: 'bukubuku', 670: 'bernyanyi', 671: 'basah', 672: 'bandara', 673: 'alasan', 674: 'akhir', 675: 'untukmu', 676: 'ujian', 677: 'tomlah', 678: 'suara', 679: 'rasanya', 680: 'potong', 681: 'pindah', 682: 'pilihan', 683: 'penuh', 684: 'pekerjaannya', 685: 'payung', 686: 'mirip', 687: 'mengira', 688: 'mengemudi', 689: 'mencobanya', 690: 'memberikan', 691: 'mematikan', 692: 'memainkan', 693: 'masalahnya', 694: 'mahasiswa', 695: 'lelucon', 696: 'lampunya', 697: 'kuda', 698: 'ketakutan', 699: 'keluarga', 700: 'kecuali', 701: 'kartu', 702: 'kakak', 703: 'jus', 704: 'jeruk', 705: 'jenis', 706: 'jendela', 707: 'jelas', 708: 'jarang', 709: 'hadir', 710: 'gula', 711: 'gedung', 712: 'digunakan', 713: 'catur', 714: 'busnya', 715: 'bumi', 716: 'beruntung', 717: 'berharap', 718: 'berasal', 719: 'bendera', 720: 'bangga', 721: 'tinggalkan', 722: 'terbesar', 723: 'terbang', 724: 'sofa', 725: 'seni', 726: 'seminggu', 727: 'sejarah', 728: 'sayang', 729: 'samudra', 730: 'rusak', 731: 'radionya', 732: 'perpustakaan', 733: 'permainan', 734: 'perjalanan', 735: 'penjara', 736: 'pekerjaanmu', 737: 'nomor', 738: 'namaku', 739: 'menyembunyikan', 740: 'mengangkat', 741: 'mendengarnya', 742: 'mencuri', 743: 'mencintainya', 744: 'memotong', 745: 'membosankan', 746: 'membersihkan', 747: 'memahami', 748: 'matahari', 749: 'mainan', 750: 'lanjut', 751: 'labalaba', 752: 'konyol', 753: 'ketinggalan', 754: 'kenal', 755: 'kemari', 756: 'keluarganya', 757: 'kelihatannya', 758: 'kehidupan', 759: 'kamus', 760: 'kamarku', 761: 'jembatan', 762: 'hilang', 763: 'hewan', 764: 'hatihati', 765: 'favoritmu', 766: 'dicuri', 767: 'dengar', 768: 'daftar', 769: 'capek', 770: 'butuhkan', 771: 'bosan', 772: 'bersenangsenang', 773: 'bayi', 774: 'asing', 775: 'akhirakhir', 776: 'usia', 777: 'untukku', 778: 'umur', 779: 'tulis', 780: 'tokyo', 781: 'tertinggi', 782: 'terkunci', 783: 'tangga', 784: 'sukai', 785: 'sengaja', 786: 'segalanya', 787: 'radio', 788: 'plastik', 789: 'pinjam', 790: 'per', 791: 'pensil', 792: 'milikku', 793: 'mengubah', 794: 'menginginkan', 795: 'menanam', 796: 'memperbaiki', 797: 'memeluk', 798: 'membicarakan', 799: 'matikan', 800: 'matanya', 801: 'matamu', 802: 'lumayan', 803: 'liburan', 804: 'lakilakiku', 805: 'kunci', 806: 'kue', 807: 'ku', 808: 'kotor', 809: 'koran', 810: 'kesempatan', 811: 'kamarmu', 812: 'jawab', 813: 'hadiah', 814: 'garam', 815: 'gagal', 816: 'empat', 817: 'diminum', 818: 'dariku', 819: 'buat', 820: 'boneka', 821: 'bingung', 822: 'binatang', 823: 'bersih', 824: 'berjanji', 825: 'barunya', 826: 'astronomi', 827: 'udara', 828: 'topi', 829: 'toilet', 830: 'tiket', 831: 'terjebak', 832: 'tas', 833: 'tanda', 834: 'tanah', 835: 'sungguh', 836: 'sudahkah', 837: 'suatu', 838: 'situ', 839: 'sepenuhnya', 840: 'seluruh', 841: 'sarung', 842: 'rok', 843: 'perusahaan', 844: 'peraturan', 845: 'pelajaran', 846: 'pekan', 847: 'nyaman', 848: 'museum', 849: 'merupakan', 850: 'menyebabkan', 851: 'menyarankan', 852: 'mengucapkan', 853: 'menghilang', 854: 'mengendarai', 855: 'mengalami', 856: 'mencintai', 857: 'memerlukan', 858: 'membuatnya', 859: 'membiarkan', 860: 'membantuku', 861: 'melewati', 862: 'melalui', 863: 'manakah', 864: 'lulus', 865: 'lewat', 866: 'langsung', 867: 'lancar', 868: 'lampu', 869: 'komputer', 870: 'kemana', 871: 'kebun', 872: 'kebakaran', 873: 'kamarnya', 874: 'kacamata', 875: 'jackson', 876: 'informasi', 877: 'hidupku', 878: 'harusnya', 879: 'ditemukan', 880: 'dinding', 881: 'dilarang', 882: 'dapatkah', 883: 'cina', 884: 'bukunya', 885: 'berusia', 886: 'berterima', 887: 'bertahan', 888: 'batu', 889: 'barang', 890: 'baju', 891: 'angkat', 892: 'anggota', 893: 'ambilkan', 894: 'akal', 895: '230', 896: '2013', 897: 'yen', 898: 'universitas', 899: 'tunjukkan', 900: 'tuanya', 901: 'tertutup', 902: 'terkadang', 903: 'terbakar', 904: 'tangannya', 905: 'tahunku', 906: 'stasiunnya', 907: 'siapapun', 908: 'setengah', 909: 'semangka', 910: 'selanjutnya', 911: 'seharian', 912: 'rusa', 913: 'ruang', 914: 'raja', 915: 'pulpen', 916: 'pisang', 917: 'pingsan', 918: 'perut', 919: 'pengacara', 920: 'pelajar', 921: 'pabrik', 922: 'olahraga', 923: 'namamu', 924: 'merindukan', 925: 'mentah', 926: 'menghubungi', 927: 'mengenalnya', 928: 'menderita', 929: 'mencurigakan', 930: 'mencium', 931: 'memberitahumu', 932: 'memberikannya', 933: 'lukisan', 934: 'lepas', 935: 'kurasa', 936: 'kuncinya', 937: 'krim', 938: 'kolam', 939: 'keberatan', 940: 'jujur', 941: 'hobiku', 942: 'hentikan', 943: 'golf', 944: 'everest', 945: 'dulunya', 946: 'duduklah', 947: 'diperbaiki', 948: 'dijual', 949: 'diam', 950: 'dengarkan', 951: 'cokelat', 952: 'botol', 953: 'bioskop', 954: 'bicarakan', 955: 'biaya', 956: 'bertengkar', 957: 'bersembunyi', 958: 'berselancar', 959: 'beritahu', 960: 'berita', 961: 'berakhir', 962: 'belok', 963: 'bebas', 964: 'bea', 965: 'bau', 966: 'bantuanmu', 967: 'bagiku', 968: 'bagian', 969: 'apinya', 970: 'antara', 971: 'anjingnya', 972: 'anaknya', 973: 'alergi', 974: 'alamat', 975: 'akibat', 976: '10', 977: 'umumnya', 978: 'umum', 979: 'uangnya', 980: 'tulisan', 981: 'tuhan', 982: 'terkena', 983: 'terdekat', 984: 'tengah', 985: 'temanmu', 986: 'teleponnya', 987: 'tambahkan', 988: 'sore', 989: 'situasi', 990: 'siapasiapa', 991: 'siapakah', 992: 'serikat', 993: 'semoga', 994: 'sembilan', 995: 'sarankan', 996: 'samasama', 997: 'salahku', 998: 'rasakan', 999: 'rapatnya'}\n"
     ]
    }
   ],
   "source": [
    "len_japan_vocab = len(french_vectorization_layer.get_vocabulary())\n",
    "index_to_word={x:y for x, y in zip(range(len_japan_vocab), french_vectorization_layer.get_vocabulary())}\n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Dataset\n",
    "dataset_len = sum(1 for _ in dataset)\n",
    "train_dataset = dataset.take(int(0.9*dataset_len))\n",
    "val_dataset = dataset.skip(int(0.9*dataset_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(units, return_sequences=True)\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w1 = tf.keras.layers.Dense(units) #buat layer encoder\n",
    "        self.w2 = tf.keras.layers.Dense(units) #buat layer decoder\n",
    "        self.v =tf.keras.layers.Dense(1) #ouput attention\n",
    "\n",
    "    def call(self, prev_dec_state, enc_state):\n",
    "        score = self.v(tf.nn.tanh(self.w2(tf.expand_dims(prev_dec_state, 1))+ self.w1(enc_state)))\n",
    "        attention_weight = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        context_vector = attention_weight*enc_state\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        return context_vector, attention_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_unit, sequence_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = BahdanauAttention(dec_unit)\n",
    "        self.gru = tf.keras.layers.GRU(vocab_size, return_state=True, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(dec_unit, activation = \"softmax\")\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def call(self, encoder, decoder, target):\n",
    "        output = []\n",
    "        attention_weight = []\n",
    "        target = self.embedding(target)\n",
    "        print(encoder)\n",
    "\n",
    "\n",
    "        for t in range(0, self.sequence_length):\n",
    "            context_vector, attention_weight = self.attention(decoder, encoder)\n",
    "            dec_input = context_vector + target[:, t]\n",
    "            output, state = self.gru(tf.expand_dim(dec_input, 1))\n",
    "            outputs.append(output[:, 0])\n",
    "\n",
    "        outputs = tf.convert_to_tensor(outputs)\n",
    "        outputs = tf.transpose(outputs, perm = [1, 0, 2])\n",
    "        outputs = self.dense(outputs)\n",
    "        return outputs, attention_weight\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 32, 256), dtype=float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"decoder_3\" (type Decoder).\n\nin user code:\n\n    File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15912\\748909649.py\", line 18, in call  *\n        context_vector, attention_weight = self.attention(decoder, encoder)\n    File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\USER\\AppData\\Local\\Temp\\__autograph_generated_file2q1_u4ir.py\", line 10, in tf__call\n        score = ag__.converted_call(ag__.ld(self).v, (ag__.converted_call(ag__.ld(tf).nn.tanh, (ag__.converted_call(ag__.ld(self).w2, (ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.ld(prev_dec_state), 1), None, fscope),), None, fscope) + ag__.converted_call(ag__.ld(self).w1, (ag__.ld(enc_state),), None, fscope),), None, fscope),), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'bahdanau_attention_3' (type BahdanauAttention).\n    \n    in user code:\n    \n        File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15912\\3508188070.py\", line 9, in call  *\n            score = self.v(tf.nn.tanh(self.w2(tf.expand_dims(prev_dec_state, 1))+ self.w1(enc_state)))\n    \n        AttributeError: 'BahdanauAttention' object has no attribute 'v'\n    \n    \n    Call arguments received by layer 'bahdanau_attention_3' (type BahdanauAttention):\n      • prev_dec_state=tf.Tensor(shape=(1, 256), dtype=float32)\n      • enc_state=tf.Tensor(shape=(None, 32, 256), dtype=float32)\n\n\nCall arguments received by layer \"decoder_3\" (type Decoder):\n  • encoder=tf.Tensor(shape=(None, 32, 256), dtype=float32)\n  • decoder=tf.Tensor(shape=(1, 256), dtype=float32)\n  • target=tf.Tensor(shape=(None, 32), dtype=int64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(FRENCH_SEQUENCE_LENGTH, ), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_UNITS, FRENCH_SEQUENCE_LENGTH)\n\u001b[1;32m---> 13\u001b[0m decoder_output, attention_weight \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHIDDEN_UNITS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m bahdanau \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel([\u001b[38;5;28minput\u001b[39m, target], decoder_input)\n\u001b[0;32m     16\u001b[0m bahdanau\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9mbv0v0d.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, encoder, decoder, target)\u001b[0m\n\u001b[0;32m     31\u001b[0m state \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m t \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconvert_to_tensor, (ag__\u001b[38;5;241m.\u001b[39mld(outputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtranspose, (ag__\u001b[38;5;241m.\u001b[39mld(outputs),), \u001b[38;5;28mdict\u001b[39m(perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9mbv0v0d.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m attention_weight\n\u001b[0;32m     24\u001b[0m t \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m---> 25\u001b[0m (context_vector, attention_weight) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m dec_input \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(context_vector) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(target)[:, ag__\u001b[38;5;241m.\u001b[39mld(t)]\n\u001b[0;32m     27\u001b[0m (output, state) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mgru, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dim, (ag__\u001b[38;5;241m.\u001b[39mld(dec_input), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2q1_u4ir.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, prev_dec_state, enc_state)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m score \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mtanh, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mw2, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(prev_dec_state), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mw1, (ag__\u001b[38;5;241m.\u001b[39mld(enc_state),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m attention_weight \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax, (ag__\u001b[38;5;241m.\u001b[39mld(score),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m     12\u001b[0m context_vector \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(attention_weight) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(enc_state)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Exception encountered when calling layer \"decoder_3\" (type Decoder).\n\nin user code:\n\n    File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15912\\748909649.py\", line 18, in call  *\n        context_vector, attention_weight = self.attention(decoder, encoder)\n    File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\USER\\AppData\\Local\\Temp\\__autograph_generated_file2q1_u4ir.py\", line 10, in tf__call\n        score = ag__.converted_call(ag__.ld(self).v, (ag__.converted_call(ag__.ld(tf).nn.tanh, (ag__.converted_call(ag__.ld(self).w2, (ag__.converted_call(ag__.ld(tf).expand_dims, (ag__.ld(prev_dec_state), 1), None, fscope),), None, fscope) + ag__.converted_call(ag__.ld(self).w1, (ag__.ld(enc_state),), None, fscope),), None, fscope),), None, fscope)\n\n    AttributeError: Exception encountered when calling layer 'bahdanau_attention_3' (type BahdanauAttention).\n    \n    in user code:\n    \n        File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15912\\3508188070.py\", line 9, in call  *\n            score = self.v(tf.nn.tanh(self.w2(tf.expand_dims(prev_dec_state, 1))+ self.w1(enc_state)))\n    \n        AttributeError: 'BahdanauAttention' object has no attribute 'v'\n    \n    \n    Call arguments received by layer 'bahdanau_attention_3' (type BahdanauAttention):\n      • prev_dec_state=tf.Tensor(shape=(1, 256), dtype=float32)\n      • enc_state=tf.Tensor(shape=(None, 32, 256), dtype=float32)\n\n\nCall arguments received by layer \"decoder_3\" (type Decoder):\n  • encoder=tf.Tensor(shape=(None, 32, 256), dtype=float32)\n  • decoder=tf.Tensor(shape=(1, 256), dtype=float32)\n  • target=tf.Tensor(shape=(None, 32), dtype=int64)"
     ]
    }
   ],
   "source": [
    "# Model Creation\n",
    "HIDDEN_UNITS=256\n",
    "EMBEDDING_DIM=256\n",
    "\n",
    "### ENCODER\n",
    "input = tf.keras.layers.Input(shape=(ENGLISH_SEQUENCE_LENGTH, ), dtype=\"int64\", name=\"input_1\")\n",
    "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_UNITS)\n",
    "encoder_output = encoder(input)\n",
    "\n",
    "### DECODER\n",
    "target = tf.keras.layers.Input(shape=(FRENCH_SEQUENCE_LENGTH, ), dtype=\"int64\", name=\"input_2\")\n",
    "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_UNITS, FRENCH_SEQUENCE_LENGTH)\n",
    "decoder_output, attention_weight = decoder(encoder_output, tf.zeros([1, HIDDEN_UNITS]), target)\n",
    "\n",
    "bahdanau = tf.keras.Model([input, target], decoder_input)\n",
    "bahdanau.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahdanau.compile(\n",
    "    optimizer = \"adam\", \n",
    "    loss = \"sparse_categorical_crossentropy\", \n",
    "    metrics =[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bahdanau.fit(train_dataset, validation_data=val_dataset, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = bahdanau.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences):\n",
    "    tokenized = english_vectorization_layer([sentences])\n",
    "    target = \"starttoken\"\n",
    "    result = \"\"\n",
    "\n",
    "    for i in range(FRENCH_SEQUENCE_LENGTH):\n",
    "        tokenized_target = french_vectorization_layer([target])\n",
    "        output = bahdanau.predict([tokenized, tokenized_target])\n",
    "        word_index =tf.argmax(output, asix = -1)[0][i].numpy()\n",
    "        word = index_to_word[word_index]\n",
    "        if word == \"endtoken\":\n",
    "            break\n",
    "        target += \" \"+word # starttoken  aku, starttoken  aku mau, starttoken  aku mau makan\n",
    "        result += \" \"+word \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
